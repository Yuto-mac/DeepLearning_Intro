{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist \n",
    "from two_layer_net import TwoLayerNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/yutowilliam/AI_house/DP_Intro/chapter04/Train_neuralnet.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yutowilliam/AI_house/DP_Intro/chapter04/Train_neuralnet.ipynb#W1sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m x_batch \u001b[39m=\u001b[39m x_train[batch_mask]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yutowilliam/AI_house/DP_Intro/chapter04/Train_neuralnet.ipynb#W1sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m t_batch \u001b[39m=\u001b[39m t_train[batch_mask]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yutowilliam/AI_house/DP_Intro/chapter04/Train_neuralnet.ipynb#W1sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m grad \u001b[39m=\u001b[39m network\u001b[39m.\u001b[39mnumerical_gradient(x_batch, t_batch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yutowilliam/AI_house/DP_Intro/chapter04/Train_neuralnet.ipynb#W1sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mW1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mb1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mW2\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mb2\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yutowilliam/AI_house/DP_Intro/chapter04/Train_neuralnet.ipynb#W1sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     network\u001b[39m.\u001b[39mparams[key] \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m learning_rate \u001b[39m*\u001b[39m grad[key]\n",
      "File \u001b[0;32m~/AI_house/DP_Intro/chapter04/two_layer_net.py:57\u001b[0m, in \u001b[0;36mTwoLayerNet.numerical_gradient\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     54\u001b[0m loss_W \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m W: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(x, t)\n\u001b[1;32m     56\u001b[0m grads \u001b[39m=\u001b[39m {}\n\u001b[0;32m---> 57\u001b[0m grads[\u001b[39m'\u001b[39m\u001b[39mW1\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m numerical_gradient(loss_W, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mW1\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     58\u001b[0m grads[\u001b[39m'\u001b[39m\u001b[39mb1\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m numerical_gradient(loss_W, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mb1\u001b[39m\u001b[39m'\u001b[39m]) \n\u001b[1;32m     59\u001b[0m grads[\u001b[39m'\u001b[39m\u001b[39mW2\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m numerical_gradient(loss_W, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mW2\u001b[39m\u001b[39m'\u001b[39m]) \n",
      "File \u001b[0;32m~/AI_house/DP_Intro/chapter04/../common/gradient.py:46\u001b[0m, in \u001b[0;36mnumerical_gradient\u001b[0;34m(f, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m fxh1 \u001b[39m=\u001b[39m f(x) \u001b[39m# f(x+h)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m x[idx] \u001b[39m=\u001b[39m tmp_val \u001b[39m-\u001b[39m h \n\u001b[0;32m---> 46\u001b[0m fxh2 \u001b[39m=\u001b[39m f(x) \u001b[39m# f(x-h)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m grad[idx] \u001b[39m=\u001b[39m (fxh1 \u001b[39m-\u001b[39m fxh2) \u001b[39m/\u001b[39m (\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mh)\n\u001b[1;32m     49\u001b[0m x[idx] \u001b[39m=\u001b[39m tmp_val \u001b[39m# 还原值\u001b[39;00m\n",
      "File \u001b[0;32m~/AI_house/DP_Intro/chapter04/two_layer_net.py:54\u001b[0m, in \u001b[0;36mTwoLayerNet.numerical_gradient.<locals>.<lambda>\u001b[0;34m(W)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnumerical_gradient\u001b[39m(\u001b[39mself\u001b[39m, x, t):\n\u001b[0;32m---> 54\u001b[0m     loss_W \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m W: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(x, t)\n\u001b[1;32m     56\u001b[0m     grads \u001b[39m=\u001b[39m {}\n\u001b[1;32m     57\u001b[0m     grads[\u001b[39m'\u001b[39m\u001b[39mW1\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m numerical_gradient(loss_W, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mW1\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/AI_house/DP_Intro/chapter04/two_layer_net.py:40\u001b[0m, in \u001b[0;36mTwoLayerNet.loss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloss\u001b[39m(\u001b[39mself\u001b[39m, x, t):\n\u001b[0;32m---> 40\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(x)\n\u001b[1;32m     42\u001b[0m     \u001b[39mreturn\u001b[39;00m cross_entropy_error(y, t)\n",
      "File \u001b[0;32m~/AI_house/DP_Intro/chapter04/two_layer_net.py:32\u001b[0m, in \u001b[0;36mTwoLayerNet.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m W1, W2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mW1\u001b[39m\u001b[39m'\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mW2\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     30\u001b[0m b1, b2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mb1\u001b[39m\u001b[39m'\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mb2\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 32\u001b[0m a1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(x, W1) \u001b[39m+\u001b[39m b1\n\u001b[1;32m     33\u001b[0m z1 \u001b[39m=\u001b[39m sigmoid(a1)\n\u001b[1;32m     34\u001b[0m a2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(z1, W2) \u001b[39m+\u001b[39m b2\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0] \n",
    "batch_size = 100 \n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "\n",
    "    # mini_batch\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
